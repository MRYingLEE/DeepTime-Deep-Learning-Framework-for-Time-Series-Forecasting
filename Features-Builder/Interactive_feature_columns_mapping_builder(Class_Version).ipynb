{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Interactive feature columns mapping builder(Class Version)",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPONrVt2qqg0",
        "colab_type": "text"
      },
      "source": [
        "# This is class version of Interactive feature columns mapping builder.\n",
        "\n",
        "Why do I want a class version instead of a Jupyter Notebook?\n",
        "\n",
        "Because finally I will create a end-to-end time series solution. Maybe an estimator class is suitable to my objectives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqlYc1rEjXWI",
        "colab_type": "text"
      },
      "source": [
        "This tools focus mapping from columns in the dataframe of Pandas to  feature columns of Tensorflow, which is thereafter used to train a model.\n",
        "\n",
        "TensorFlow provides many types of feature columns. \n",
        "You may visit https://www.tensorflow.org/tutorials/structured_data/feature_columns to know the detail.\n",
        "\n",
        "Please note this tools support very limited feature column types. The generated code could be limited also, but you may modify it. BTW, a lambda statement can be used to deal with data preprocessing.\n",
        "\n",
        "I am working on Deep Time (https://github.com/MRYingLEE/DeepTime-Deep-Learning-Framework-for-Time-Series-Forecasting). This tools is part of my research work.\n",
        "\n",
        "Tensorflow 2.x is used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VxyBFc_kKazA"
      },
      "source": [
        "# Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjnV1NJswrwp",
        "colab_type": "text"
      },
      "source": [
        "Maybe later sklearn Preprocessing function (https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing) will be supported.\n",
        "\n",
        "So far, only train_test_split of sklearn is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LuOWVJBz8a6G",
        "colab": {}
      },
      "source": [
        "!pip install sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9dEreb4QKizj",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.feature_column import *\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from io import StringIO\n",
        "# # Import ipywidgets\n",
        "\n",
        "# ipywidgets （https://github.com/jupyter-widgets/ipywidgets） makes the Jupyter Notebook interactive.\n",
        "\n",
        "from ipywidgets import GridspecLayout\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import Button, Layout, jslink, IntText, IntSlider\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNzoBcOtR62x",
        "colab_type": "text"
      },
      "source": [
        "# Useful helper functions for feature columns\n",
        "\n",
        "The reason I create some helper function is that I want to make the generated code short and easy to read."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RusCoppU78Mi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to generate a one-hot column by the vocabulary list.\n",
        "def categorical_strings(column,vocabulary_list):\n",
        "  sparse_column = feature_column.categorical_column_with_vocabulary_list(\n",
        "      column, vocabulary_list)\n",
        "  one_hot_column = feature_column.indicator_column(sparse_column)\n",
        "  return one_hot_column\n",
        "\n",
        "# A function to generate an embedding column by the vocabulary list.\n",
        "def categorical_strings_embedding(column,vocabulary_list, embedding_dim=8):\n",
        "  sparse_column = feature_column.categorical_column_with_vocabulary_list(\n",
        "      column, vocabulary_list)\n",
        "  embedding_column = feature_column.embedding_column(sparse_column, dimension=embedding_dim)\n",
        "  return embedding_column\n",
        "\n",
        "# A function to generate a hashed column by the vocabulary list.\n",
        "def categorical_hash(column,vocabulary_list, bucket_size=1000):\n",
        "  hashed = feature_column.categorical_column_with_hash_bucket(\n",
        "      column, hash_bucket_size=bucket_size)\n",
        "  hashed=feature_column.indicator_column(hashed)\n",
        "  return hashed\n",
        "\n",
        "# A function to generate a one-hot column by the vocabulary list for an integer column.\n",
        "def categorical_identitys(column,vocabulary_list):\n",
        "  min_int=np.min(vocabulary_list)\n",
        "  max_int=np.max(vocabulary_list)\n",
        "  count_v=len(vocabulary_list)\n",
        "\n",
        "  if ((min_int<0) or (max_int>20)):\n",
        "    sparse_column = feature_column.categorical_column_with_hash_bucket(\n",
        "      column, count_v, dtype=tf.dtypes.int32)\n",
        "  else:\n",
        "    sparse_column = feature_column.categorical_column_with_identity(\n",
        "      column, max_int)\n",
        "    \n",
        "  one_hot_column = feature_column.indicator_column(sparse_column)\n",
        "  return one_hot_column\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP_WZQa--p2N",
        "colab_type": "text"
      },
      "source": [
        "# Class of an Estimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_mJwgD2-t1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TsEstimator:\n",
        "## The feature column types\n",
        "    # Here is a full list of built-in features of tensorflow 2.\n",
        "    # But actually not all are supported in this tools.\n",
        "  feature_kinds={\n",
        "      \"bucketized_column(...)\":\"Represents discretized dense input bucketed by boundaries.\",\n",
        "      \"categorical_column_with_hash_bucket(...)\":\"Represents sparse feature where ids are set by hashing.\",\n",
        "      \"categorical_column_with_identity(...)\":\"A CategoricalColumn that returns identity values.\",\n",
        "      \"categorical_column_with_vocabulary_file(...)\":\"A CategoricalColumn with a vocabulary file.\",\n",
        "      \"categorical_column_with_vocabulary_list(...)\":\"A CategoricalColumn with in-memory vocabulary.\",\n",
        "      \"crossed_column(...)\":\"Returns a column for performing crosses of categorical features.\",\n",
        "      \"embedding_column(...)\":\"DenseColumn that converts from sparse, categorical input.\",\n",
        "      \"indicator_column(...)\":\"Represents multi-hot representation of given categorical column.\",\n",
        "      \"make_parse_example_spec(...)\":\"Creates parsing spec dictionary from input feature_columns.\",\n",
        "      \"numeric_column(...)\":\"Represents real valued or numerical features.\",\n",
        "      \"sequence_categorical_column_with_hash_bucket(...)\":\"A sequence of categorical terms where ids are set by hashing.\",\n",
        "      \"sequence_categorical_column_with_identity(...)\":\"Returns a feature column that represents sequences of integers.\",\n",
        "      \"sequence_categorical_column_with_vocabulary_file(...)\":\"A sequence of categorical terms where ids use a vocabulary file.\",\n",
        "      \"sequence_categorical_column_with_vocabulary_list(...)\":\"A sequence of categorical terms where ids use an in-memory list.\",\n",
        "      \"sequence_numeric_column(...)\":\"Returns a feature column that represents sequences of numeric data.\",\n",
        "      \"shared_embeddings(...)\":\"List of dense columns that convert from sparse, categorical input.\",\n",
        "      \"weighted_categorical_column(...)\":\"Applies weight values to a CategoricalColumn.\",\n",
        "      \"?\":\"Unknown\"\n",
        "    }\n",
        "    # ## The default feature kind for dtype of Pandas\n",
        "\n",
        "    # For every dtype of Pandas, a default feature kind is assigned.\n",
        "\n",
        "  dtype_default_feature={\n",
        "      \"object\":\"?\",\n",
        "      \"int64\":\"numeric_column(...)\",\n",
        "      \"float64\":\"numeric_column(...)\",\n",
        "      \"bool\":\"numeric_column(...)\",\n",
        "      \"datetime64\":\"?\",\n",
        "      \"timedelta[ns]\":\"?\",\n",
        "      \"category\":\"categorical_strings(...)\"\n",
        "    }\n",
        "\n",
        "      # ## The available feature_column for dtype of Pandas\n",
        "\n",
        "    # This is a dictionary of the matching of dtype and feature kinds.\n",
        "\n",
        "    # Some adavanced feature kinds are disabled here.\n",
        "\n",
        "\n",
        "  dtype_features_cross = StringIO(\"\"\"Kind,object,int64,float64,bool,datetime64,timedelta[ns],category,cat_int64,cat_string\n",
        "    bucketized_column(...),FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE\n",
        "    categorical_column_with_hash_bucket(...),FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE\n",
        "    categorical_column_with_identity(...),FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE\n",
        "    categorical_column_with_vocabulary_file(...),FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE\n",
        "    categorical_column_with_vocabulary_list(...),FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE\n",
        "    crossed_column(...),FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE\n",
        "    embedding_column(...),TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE\n",
        "    indicator_column(...),FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE\n",
        "    make_parse_example_spec(...),FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE\n",
        "    numeric_column(...),FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE\n",
        "    sequence_categorical_column_with_hash_bucket(...),TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE\n",
        "    sequence_categorical_column_with_identity(...),TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE\n",
        "    sequence_categorical_column_with_vocabulary_file(...),TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE\n",
        "    sequence_categorical_column_with_vocabulary_list(...),TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE\n",
        "    sequence_numeric_column(...),TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE\n",
        "    shared_embeddings(...),FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE\n",
        "    weighted_categorical_column(...),FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE\n",
        "    categorical_identitys,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE\n",
        "    categorical_strings,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE\n",
        "        \"\"\")\n",
        "  df_dtype_features_cross = pd.read_csv(dtype_features_cross, sep=\",\")\n",
        "\n",
        "  def __init__(self, df_all, df_train, df_val=None, df_test=None, categories_limit=20):\n",
        "    self._df_all=df_all\n",
        "\n",
        "    assert(df_all is not None)\n",
        "\n",
        "    if (df_train is None):\n",
        "      self._df_train=df_all\n",
        "    else:\n",
        "      self._df_train=df_train\n",
        "    self._df_val=df_val\n",
        "    self._df_df_test=df_test\n",
        "\n",
        "    self.input_features=[]\n",
        "    self.label_features=[]\n",
        "\n",
        "    self.global_normalizers={} # Not used so far \n",
        "    self.categorical_columns=[]\n",
        "    self.categories_limit=categories_limit\n",
        "    self.grid=None\n",
        "    self.category_lists= self.__df_desc()\n",
        "      # If a column has less than this number (20 as default) of unique value, I will treate it as a category column.\n",
        "\n",
        "  @classmethod\n",
        "  def get_available_features(cls,col_dtype):\n",
        "    return set(cls.df_dtype_features_cross[[\"Kind\",col_dtype]][cls.df_dtype_features_cross[col_dtype]][\"Kind\"].unique())\n",
        "\n",
        "    # ## To generate normalizer lambda and denormalizer one\n",
        "\n",
        "    # So far, only 2 kinds of normalizer and denormalizer are supported:\n",
        "\n",
        "  # min-max  : (value-min)/(max-min)\n",
        "  # To generate min-max normalizer and denomalizer lambda statements\n",
        "  @staticmethod\n",
        "  def min_max_normalizer(min_v,max_v, v_str=\"by_train\",is_int64=False):\n",
        "    if is_int64:\n",
        "      ext_v_str=\"tf.cast(\"+v_str+\",tf.float32)\"\n",
        "    else:\n",
        "      ext_v_str=v_str\n",
        "    \n",
        "    return \"lambda \"+v_str+\": (\"+ext_v_str+ \" -\"+str(min_v)+\")/(\"+str(max_v)+\"-\"+str(min_v)+\")\",\"lambda \"+v_str+\": \"+ext_v_str+ \" *(\"+str(max_v)+\"-\"+str(min_v)+\")+\"+str(min_v)\n",
        "\n",
        "  # mean-std  : (value-mean)/std\n",
        "  # To generate mean-std normalizer and denomalizer lambda statements\n",
        "  @staticmethod\n",
        "  def std_normalizer(v_mean,v_std, v_str=\"by_train\",is_int64=False):\n",
        "    if is_int64:\n",
        "      ext_v_str=\"tf.cast(\"+v_str+\",tf.float32)\"\n",
        "    else:\n",
        "      ext_v_str=v_str\n",
        "\n",
        "    return \"lambda \"+v_str+\": (\"+ext_v_str+ \" -\"+str(v_mean)+\")/\"+str(v_std),\"lambda \"+v_str+\": \"+ext_v_str+ \" *\"+str(v_std)+\"+\"+str(v_mean)\n",
        "\n",
        "  # To generate min-max/mean-std normalizer and denomalizer lambda statements given an statistics data\n",
        "  @staticmethod\n",
        "  def create_local_normalizers(col_name,df_statistics, v_str=\"by_train\",is_int64=False):\n",
        "    v_min=df_statistics.loc[col_name][\"min\"]\n",
        "    v_max=df_statistics.loc[col_name][\"max\"]\n",
        "    v_mean=df_statistics.loc[col_name][\"mean\"]\n",
        "    v_std=df_statistics.loc[col_name][\"std\"]\n",
        "\n",
        "    n1,d1=TsEstimator.min_max_normalizer(v_min,v_max,v_str,is_int64=is_int64)\n",
        "    n2,d2=TsEstimator.std_normalizer(v_mean,v_std,v_str,is_int64=is_int64)\n",
        "\n",
        "    locals={n1:d1,n2:d2}\n",
        "    return locals\n",
        "\n",
        "  # To generated a suitable string for an integer list\n",
        "  @staticmethod\n",
        "  def int_list_as_string(a):\n",
        "    s = [str(i) for i in a]\n",
        "    return  \"[\"+\",\".join(s)+\"]\"\n",
        "\n",
        "  # To generated a suitable string for a string list\n",
        "  @staticmethod\n",
        "  def string_list_as_string(s):\n",
        "    return  \"['\"+\"','\".join(s)+\"']\"\n",
        "\n",
        "    # ## To generate available feature kinds and suitable normalizer lambda statements for every column.\n",
        "\n",
        "  # Please note the whole dataframe and the train part are both required.\n",
        "\n",
        "  # The whole dataframe is used to decide the vocalbulary list for each column.\n",
        "\n",
        "  # Both the whole dataframe and the train part are used to generate lambda statements for NUMERIC columns. So normalizing can be based on the whole data or only the train part. It's up to the data scientist.\n",
        "\n",
        "  def __df_desc(self):\n",
        "    df_all=self._df_all\n",
        "    df_train=self._df_train\n",
        "\n",
        "    df_statistics_train=df_train.describe().T # I use train part to normalize!\n",
        "    df_statistics_all=df_all.describe().T # I use train part to normalize!\n",
        "    \n",
        "    category_lists={}\n",
        "    \n",
        "    for c in df_train.columns:\n",
        "      dtype_name=df_train[c].dtype.name\n",
        "\n",
        "      availables=self.get_available_features(dtype_name)\n",
        "\n",
        "      if availables is None:\n",
        "        availables={}\n",
        "\n",
        "      feature=\"numeric_column('\"+c+\"')\"\n",
        "\n",
        "      local_normalizers={}\n",
        "\n",
        "      if ((dtype_name==\"int64\") or (dtype_name==\"object\")):\n",
        "        is_int64=(dtype_name==\"int64\")\n",
        "\n",
        "        values_unique=df_all[c].unique()\n",
        "        f=len(values_unique)   # I use all rows to decide the cetegory list   \n",
        "        if f<self.categories_limit: #Category\n",
        "          if is_int64:\n",
        "            feature=categorical_identitys.__name__+\"('\"+c+\"',\"+self.int_list_as_string(values_unique)+\")\"\n",
        "          else:\n",
        "            feature=categorical_strings.__name__+\"('\"+c+\"',\"+self.string_list_as_string(values_unique)+\")\"\n",
        "          self.categorical_columns.append(c)\n",
        "        else:\n",
        "          if is_int64:\n",
        "            feature=\"numeric_column('\"+c+\"')\"\n",
        "            local_normalizers=self.create_local_normalizers(c,df_statistics_train,v_str=\"by_train\", is_int64=True)\n",
        "            self.global_normalizers.update(local_normalizers)\n",
        "            local_normalizers1=self.create_local_normalizers(c,df_statistics_all,v_str=\"by_all\", is_int64=True)\n",
        "            self.global_normalizers.update(local_normalizers1)\n",
        "            local_normalizers.update(local_normalizers1)\n",
        "          else:\n",
        "            feature=\"embedding_column('\"+\"('\"+c+\"')\"\n",
        "      else:\n",
        "        if (dtype_name==\"float64\"):\n",
        "            feature=\"numeric_column('\"+c+\"')\"\n",
        "            local_normalizers=self.create_local_normalizers(c,df_statistics_train,v_str=\"by_train\", is_int64=False)\n",
        "            self.global_normalizers.update(local_normalizers)\n",
        "            local_normalizers1=self.create_local_normalizers(c,df_statistics_all,v_str=\"by_all\", is_int64=False)\n",
        "            self.global_normalizers.update(local_normalizers1)\n",
        "            local_normalizers.update(local_normalizers1)\n",
        "        elif  (dtype_name==\"bool\"):\n",
        "            feature=\"numeric_column('\"+c+\"')\"\n",
        "        elif (dtype_name==\"category\"):\n",
        "          feature=\"categorical_column_with_vocabulary_list('\"+\"('\"+c+\"')\"\n",
        "          self.categorical_columns.append(c)\n",
        "        else:\n",
        "          feature=dtype_defaults[dtype_name] \n",
        "      \n",
        "      availables.add(feature)\n",
        "\n",
        "      availables={s.replace(\"(...)\",\"('\"+c+\"')\") for s in availables}\n",
        "      category_lists[c]={\"default\":feature,\"available\":availables,\"normalizers\": local_normalizers}\n",
        "\n",
        "    return category_lists\n",
        "\n",
        "\n",
        "  def get_feature_grid(self,default_inputs=[], default_labels=[]):\n",
        "    if self.grid is not None:\n",
        "      return self.grid\n",
        "\n",
        "    # category_lists=df_desc(df_all,df_train)\n",
        "    df_all=self._df_all\n",
        "    df_train=self._df_train\n",
        "\n",
        "    cols=len(df_train.columns)\n",
        "    grid = GridspecLayout(cols+1, 12)\n",
        "    # To add a header at row 0\n",
        "    grid[0,0]= widgets.Label(value=\"Column\")\n",
        "    grid[0,1]= widgets.Label(value=\"dtype\")\n",
        "    grid[0,2]= widgets.Label(value=\"Input?\")\n",
        "    grid[0,3]= widgets.Label(value=\"Label?\")\n",
        "    grid[0,4:7]= widgets.Label(value=\"Feature Kind\")\n",
        "    grid[0,8:]= widgets.Label(value=\"Numeric Normalizer\")\n",
        "\n",
        "    for i in range(cols):\n",
        "      feature_option=self.category_lists[df_train.columns[i]]\n",
        "      grid[i+1,0]= widgets.Label(value=df_train.columns[i])\n",
        "      grid[i+1,1]= widgets.Label(value=df_train.dtypes[i].name)\n",
        "      grid[i+1,2]=widgets.Checkbox(value=(df_train.columns[i] in default_inputs),description='',indent=False,layout=Layout(height='auto', width='auto'))\n",
        "      grid[i+1,3]=widgets.Checkbox(value=(df_train.columns[i] in default_labels),indent=False,description='',layout=Layout(height='auto', width='auto'))\n",
        "      \n",
        "      grid[i+1,4:7]= widgets.Dropdown(\n",
        "        options=list(feature_option['available']),\n",
        "        value=feature_option['default'],\n",
        "        description=\"\",\n",
        "        layout=Layout(height='auto', width='auto')\n",
        "        )\n",
        "      \n",
        "      if len(feature_option['normalizers'])>0:\n",
        "        grid[i+1,8:]=widgets.Dropdown(\n",
        "          options=list(feature_option['normalizers'].keys()),\n",
        "          value=list(feature_option['normalizers'].keys())[0],\n",
        "          layout=Layout(height='auto', width='auto'),\n",
        "          description=\"\"\n",
        "          )\n",
        "    \n",
        "    self.grid=grid\n",
        "\n",
        "    return grid\n",
        "\n",
        "    # To generate code based on interactive grid\n",
        "  def __generate_code(self):\n",
        "    code_generator=[]\n",
        "    feature_inputs=[]\n",
        "    feature_labels=[]\n",
        "\n",
        "    grid=self.grid\n",
        "    for i in range(1,grid.n_rows):\n",
        "      f_col=grid[i,4].value\n",
        "      # print(f_col)\n",
        "      if (grid[i,4].value.startswith(\"numeric_column(\") and (grid[i,1].value !=\"bool\")):\n",
        "        f_col=f_col[:-1]\n",
        "\n",
        "        if (grid[i,2].value==True):\n",
        "          code_generator.append(\"input_features.append(\"+f_col+\",normalizer_fn=\"+grid[i,8].value+\"))\")\n",
        "          feature_inputs.append(grid[i,0].value)\n",
        "        if (grid[i,3].value==True):\n",
        "          code_generator.append(\"label_features.append(\"+f_col+\",normalizer_fn=\"+grid[i,8].value+\"))\")\n",
        "          feature_labels.append(grid[i,0].value)\n",
        "      else:\n",
        "        if (grid[i,2].value==True):\n",
        "          code_generator.append(\"input_features.append(\"+f_col+\")\")\n",
        "          feature_inputs.append(grid[i,0].value)\n",
        "        if (grid[i,3].value==True):\n",
        "          code_generator.append(\"label_features.append(\"+f_col+\")\")\n",
        "          feature_labels.append(grid[i,0].value)\n",
        "    return code_generator, feature_labels    \n",
        "\n",
        "  def __run_generated_code(self, code_generator):\n",
        "    code=';'.join(code_generator)\n",
        "    # print(code)\n",
        "\n",
        "    try:\n",
        "      self.input_features.clear()\n",
        "      self.label_features.clear()\n",
        "      exec(code,None, {'input_features':self.input_features,'label_features':self.label_features})\n",
        "      print(\"The feature_columns have been generated!\")\n",
        "    except:\n",
        "      print(\"Please check the generated code\", sys.exc_info()[0])\n",
        "    # print(code_generator)\n",
        "\n",
        "  def update_by_grid(self):\n",
        "    code,_ =self.__generate_code()\n",
        "    # print(\"code:\",code)\n",
        "    self.__run_generated_code(code)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zVh-ESps9u1",
        "colab_type": "text"
      },
      "source": [
        "## <font color=red> Your Dataframe here</font>\n",
        "Typically, this is the <font color=red>ONLY</font> place for you to type.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0QmMhPbyBxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csvURL = '' # the csv data file or web path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvyRfT11MUpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "Testing=True # INTERNAL for me to debug. You don't need to care about this."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYTERy0Js85O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframe=None\n",
        "default_inputs=[]  # The default features list for inputs\n",
        "default_labels=[] # The default features list for labels\n",
        "\n",
        "if (csvURL!=''):\n",
        "  dataframe = pd.read_csv(csvURL)\n",
        "  dataframe.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG_6nILluUe_",
        "colab_type": "text"
      },
      "source": [
        "## A demo dataframe if you don't create one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Med8aJxvveTh",
        "colab_type": "text"
      },
      "source": [
        "I will use a small [dataset](https://archive.ics.uci.edu/ml/datasets/heart+Disease) provided by the Cleveland Clinic Foundation for Heart Disease. There are several hundred rows in the CSV. Each row describes   a patient, and each column describes an attribute.<br>\n",
        "Notice there are both numeric (including bool) and categorical columns.\n",
        "\n",
        ">Column| Description| Feature Type | Data Type\n",
        ">------------|--------------------|----------------------|-----------------\n",
        ">Age | Age in years | Numerical | integer\n",
        ">Sex | (1 = male; 0 = female) | Categorical | integer\n",
        ">CP | Chest pain type (0, 1, 2, 3, 4) | Categorical | integer\n",
        ">Trestbpd | Resting blood pressure (in mm Hg on admission to the hospital) | Numerical | integer\n",
        ">Chol | Serum cholestoral in mg/dl | Numerical | integer\n",
        ">FBS | (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) | Categorical | integer\n",
        ">RestECG | Resting electrocardiographic results (0, 1, 2) | Categorical | integer\n",
        ">Thalach | Maximum heart rate achieved | Numerical | integer\n",
        ">Exang | Exercise induced angina (1 = yes; 0 = no) | Categorical | integer\n",
        ">Oldpeak | ST depression induced by exercise relative to rest | Numerical | float\n",
        ">Slope | The slope of the peak exercise ST segment | Numerical | integer\n",
        ">CA | Number of major vessels (0-3) colored by flourosopy | Numerical | integer\n",
        ">Thal | 3 = normal; 6 = fixed defect; 7 = reversable defect | Categorical | string\n",
        ">Target | Diagnosis of heart disease (1 = true; 0 = false) | Classification | integer\n",
        ">is_male | Whether a person is male (true or false) | Numerical | bool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "REZ57BXCLdfG",
        "colab": {}
      },
      "source": [
        "\n",
        "if (dataframe is None):\n",
        "  csvURL = 'https://storage.googleapis.com/applied-dl/heart.csv'\n",
        "  labels='target'\n",
        "  dataframe = pd.read_csv(csvURL)\n",
        "  dataframe['is_male']=(dataframe['sex']==0) # As a demo of a column of bool\n",
        "  default_inputs=['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca','is_male']  # The default features list for inputs\n",
        "  default_labels=['target'] # The default features list for labels\n",
        "\n",
        "dataframe.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDQ9-v7WxJp7",
        "colab_type": "text"
      },
      "source": [
        "## Split data into Train and Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YEOpw7LhMYsI",
        "colab": {}
      },
      "source": [
        "dataframe_train, dataframe_test = train_test_split(dataframe, test_size=0.2)\n",
        "dataframe_train, dataframe_val = train_test_split(dataframe_train, test_size=0.2)\n",
        "print(len(dataframe_train), 'train examples')\n",
        "print(len(dataframe_val), 'validation examples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqPxNqGNQpLN",
        "colab_type": "text"
      },
      "source": [
        "## Create an estimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "580l-ZQhPP5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator=TsEstimator(dataframe, dataframe_train, dataframe_val, dataframe_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IWdQBbSMXUL",
        "colab_type": "text"
      },
      "source": [
        "## Inspect the data by categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73VFzUHpMYph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use seaborn for pairplot\n",
        "!pip install -q seaborn\n",
        "# import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# plt.figure(figsize=(20,5))\n",
        "sns.pairplot(dataframe[estimator.categorical_columns], diag_kind=\"kde\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7BshfvV0X1g",
        "colab_type": "text"
      },
      "source": [
        "## To create an interactive grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CZLvOxXRn7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0grSPtD7QBv",
        "colab_type": "text"
      },
      "source": [
        "You may try the builder INTERACTIVELY."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_SWf5pRPYtn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid=estimator.get_feature_grid(default_inputs, default_labels)\n",
        "grid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj0RBVpb7b6N",
        "colab_type": "text"
      },
      "source": [
        "<font color=red>**RERUN** the following cells once you change the above settings.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PIYVldRC6sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator.update_by_grid()\n",
        "\n",
        "# assert(len(estimator.code_generator)>0)\n",
        "\n",
        "# code_generator\n",
        "estimator.input_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "84ef46LXMfvu"
      },
      "source": [
        "## Create an input pipeline using tf.data\n",
        "\n",
        "Next, I will wrap the dataframes with [tf.data](https://www.tensorflow.org/guide/datasets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NkcaMYP-MsRe",
        "colab": {}
      },
      "source": [
        "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
        "def df_to_dataset(dataframe,input_cols, label_cols, shuffle=True, batch_size=32):\n",
        "  labels = dataframe[label_cols]\n",
        "  dataframe= dataframe[input_cols]\n",
        "\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CXbbXkJvMy34",
        "colab": {}
      },
      "source": [
        "batch_size = 5 # A small batch sized is used for demonstration purposes\n",
        "train_ds = df_to_dataset(dataframe_train,feature_inputs,feature_labels, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(dataframe_val,feature_inputs, feature_labels,shuffle=False, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(dataframe_test,feature_inputs, feature_labels,shuffle=False, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qRLGSMDzM-dl"
      },
      "source": [
        "## Understand the input pipeline\n",
        "\n",
        "Now that I have created the input pipeline, let's call it to see the format of the data it returns. I have used a small batch size to keep the output readable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CSBo3dUVNFc9",
        "colab": {}
      },
      "source": [
        "for feature_batch, label_batch in train_ds.take(1):\n",
        "  print('Every feature:', list(feature_batch.keys()))\n",
        "  print('A batch of ages:', feature_batch['age'])\n",
        "  print('A batch of targets:', label_batch )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OT5N6Se-NQsC"
      },
      "source": [
        "The dataset returns a dictionary of column names (from the dataframe) that map to column values from rows in the dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FWlDGwLzboP",
        "colab_type": "text"
      },
      "source": [
        "## Test the features mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9YMJYbRPhfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "next(iter(train_ds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mxwiHFHuNhmf",
        "colab": {}
      },
      "source": [
        "# I will use this batch to demonstrate several types of feature columns\n",
        "example_batch = next(iter(train_ds))[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gl49oP1eOGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_G2iZ1k8hHk",
        "colab_type": "text"
      },
      "source": [
        "## Every feature mapping can be tested"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0wfLB8Q3N3UH",
        "colab": {}
      },
      "source": [
        "if Testing:\n",
        "  for f in estimator.input_features:\n",
        "    print(f)\n",
        "    feature_layer = layers.DenseFeatures(f,dtype='float64' )\n",
        "    print(feature_layer(example_batch).numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M-nDp8krS_ts"
      },
      "source": [
        "## Create a feature layer\n",
        "Now that I have defined the feature columns, I will use a [DenseFeatures](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/DenseFeatures) layer to input them to our Keras model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6o-El1R2TGQP",
        "colab": {}
      },
      "source": [
        "feature_layer = tf.keras.layers.DenseFeatures(estimator.input_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bBx4Xu0eTXWq"
      },
      "source": [
        "## Create, compile, and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_YJPPb3xTPeZ",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  feature_layer,\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_ds,\n",
        "          validation_data=val_ds,\n",
        "          epochs=5)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}