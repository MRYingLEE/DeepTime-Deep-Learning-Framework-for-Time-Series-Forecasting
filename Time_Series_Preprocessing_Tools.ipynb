{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Time Series Preprocessing Tools",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPONrVt2qqg0",
        "colab_type": "text"
      },
      "source": [
        "# This is a preprocessing tools for time series\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqlYc1rEjXWI",
        "colab_type": "text"
      },
      "source": [
        "This tools focus \n",
        "\n",
        "1. mapping from columns in the dataframe of Pandas to  values by \n",
        "  (1) One-hot coding; \n",
        "  (2) Categorical indicator\n",
        "  (3) normalizing functions.\n",
        "\n",
        "2. Windowing historic data automatically, so that RNN model can be fitted.\n",
        "\n",
        "\n",
        "\n",
        "I am working on Deep Time (https://github.com/MRYingLEE/DeepTime-Deep-Learning-Framework-for-Time-Series-Forecasting). This tools is part of my research work.\n",
        "\n",
        "Tensorflow 2.x is used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFCT4ePCedWC",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://www.tensorflow.org/tutorials/structured_data/images/time_series.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VxyBFc_kKazA"
      },
      "source": [
        "# Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjnV1NJswrwp",
        "colab_type": "text"
      },
      "source": [
        "Maybe later sklearn Preprocessing function (https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing) will be supported.\n",
        "\n",
        "So far, only train_test_split of sklearn is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LuOWVJBz8a6G",
        "colab": {}
      },
      "source": [
        "!pip install sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9dEreb4QKizj",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.feature_column import *\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from io import StringIO\n",
        "\n",
        "# ipywidgets （https://github.com/jupyter-widgets/ipywidgets） makes the Jupyter Notebook interactive.\n",
        "from ipywidgets import *\n",
        "\n",
        "import sys\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tY_iNI1SLUWB"
      },
      "source": [
        "# Useful helper functions for Map transformation\n",
        "\n",
        "The reason I create some helper function is that I want to make the generated code short and easy to read."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r8uxeC7NLUWI",
        "colab": {}
      },
      "source": [
        "# A function to generate a one-hot column by the vocabulary list.\n",
        "def categorical_strings(column,vocabulary_list):\n",
        "  def one_hot_column(row):\n",
        "    count_v=len(vocabulary_list)\n",
        "\n",
        "    table = tf.lookup.StaticVocabularyTable(\n",
        "      tf.lookup.KeyValueTensorInitializer(\n",
        "      vocabulary_list, range(count_v), key_dtype=tf.string, value_dtype=tf.int64, name=column\n",
        "      ),\n",
        "      1)\n",
        "\n",
        "    out = table.lookup(row)\n",
        "\n",
        "    return tf.one_hot(out,count_v+1)\n",
        "\n",
        "  return column, one_hot_column\n",
        "\n",
        "# # A function to generate an embedding column by the vocabulary list.\n",
        "# def categorical_strings_embedding(column,vocabulary_list, embedding_dim=8):\n",
        "#   sparse_column = feature_column.categorical_column_with_vocabulary_list(\n",
        "#       column, vocabulary_list)\n",
        "#   embedding_column = feature_column.embedding_column(sparse_column, dimension=embedding_dim)\n",
        "#   return embedding_column\n",
        "\n",
        "# # A function to generate a hashed column by the vocabulary list.\n",
        "# def categorical_hash(column,vocabulary_list, bucket_size=1000):\n",
        "#   hashed = feature_column.categorical_column_with_hash_bucket(\n",
        "#       column, hash_bucket_size=bucket_size)\n",
        "#   hashed=feature_column.indicator_column(hashed)\n",
        "#   return hashed\n",
        "\n",
        "# A function to generate a one-hot column by the vocabulary list for an integer column.\n",
        "def categorical_identitys(column,vocabulary_list):\n",
        "  def one_hot_column(row):\n",
        "    count_v=len(vocabulary_list)\n",
        "\n",
        "    table = tf.lookup.StaticVocabularyTable(\n",
        "      tf.lookup.KeyValueTensorInitializer(\n",
        "      vocabulary_list, range(count_v), key_dtype=tf.int64, value_dtype=tf.int64, name=column\n",
        "      ),\n",
        "      1)\n",
        "\n",
        "    out = table.lookup(row)\n",
        "\n",
        "    return tf.one_hot(out,count_v+1)\n",
        "\n",
        "  return column, one_hot_column"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP_WZQa--p2N",
        "colab_type": "text"
      },
      "source": [
        "# Class of an Estimator\n",
        "\n",
        "I like the idea of estimator to make machine learning more easily, but this is NOT an child of tf.estimator.Estimator class(https://www.tensorflow.org/guide/estimator).\n",
        "\n",
        "tf.estimator.Estimator class depends on feature_column (https://www.tensorflow.org/api_docs/python/tf/feature_column) heavily. I like the idea of feature_column also, but feature_column doesn't work well with time series and feature_column doeen't support functional API of Keras well. (If you find a way to solve my headache, please let me know.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_mJwgD2-t1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TsEstimator:\n",
        "## The feature column types\n",
        "    # Here is a full list of built-in features of tensorflow 2.\n",
        "    # But actually not all are supported in this tools.\n",
        "  feature_kinds={\n",
        "      \"bucketized_column(...)\":\"Represents discretized dense input bucketed by boundaries.\",\n",
        "      \"categorical_column_with_hash_bucket(...)\":\"Represents sparse feature where ids are set by hashing.\",\n",
        "      \"categorical_column_with_identity(...)\":\"A CategoricalColumn that returns identity values.\",\n",
        "      \"categorical_column_with_vocabulary_file(...)\":\"A CategoricalColumn with a vocabulary file.\",\n",
        "      \"categorical_column_with_vocabulary_list(...)\":\"A CategoricalColumn with in-memory vocabulary.\",\n",
        "      \"crossed_column(...)\":\"Returns a column for performing crosses of categorical features.\",\n",
        "      \"embedding_column(...)\":\"DenseColumn that converts from sparse, categorical input.\",\n",
        "      \"indicator_column(...)\":\"Represents multi-hot representation of given categorical column.\",\n",
        "      \"make_parse_example_spec(...)\":\"Creates parsing spec dictionary from input feature_columns.\",\n",
        "      \"numeric_column(...)\":\"Represents real valued or numerical features.\",\n",
        "      \"sequence_categorical_column_with_hash_bucket(...)\":\"A sequence of categorical terms where ids are set by hashing.\",\n",
        "      \"sequence_categorical_column_with_identity(...)\":\"Returns a feature column that represents sequences of integers.\",\n",
        "      \"sequence_categorical_column_with_vocabulary_file(...)\":\"A sequence of categorical terms where ids use a vocabulary file.\",\n",
        "      \"sequence_categorical_column_with_vocabulary_list(...)\":\"A sequence of categorical terms where ids use an in-memory list.\",\n",
        "      \"sequence_numeric_column(...)\":\"Returns a feature column that represents sequences of numeric data.\",\n",
        "      \"shared_embeddings(...)\":\"List of dense columns that convert from sparse, categorical input.\",\n",
        "      \"weighted_categorical_column(...)\":\"Applies weight values to a CategoricalColumn.\",\n",
        "      \"?\":\"Unknown\"\n",
        "    }\n",
        "    # ## The default feature kind for dtype of Pandas\n",
        "\n",
        "    # For every dtype of Pandas, a default feature kind is assigned.\n",
        "\n",
        "  dtype_default_feature={\n",
        "      \"object\":\"?\",\n",
        "      \"int64\":\"numeric_column(...)\",\n",
        "      \"float64\":\"numeric_column(...)\",\n",
        "      \"bool\":\"numeric_column(...)\",\n",
        "      \"datetime64\":\"?\",\n",
        "      \"timedelta[ns]\":\"?\",\n",
        "      \"category\":\"categorical_strings(...)\"\n",
        "    }\n",
        "\n",
        "  dtype_features_cross = StringIO(\"\"\"Kind,object,int64,float64,bool,datetime64,timedelta[ns],category,cat_int64,cat_string\n",
        "    categorical_identitys,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE\n",
        "    categorical_strings,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE\n",
        "        \"\"\")\n",
        "  df_dtype_features_cross = pd.read_csv(dtype_features_cross, sep=\",\")\n",
        "\n",
        "  def __init__(self, df_all, df_train, df_val=None, df_test=None, categories_limit=20):\n",
        "    self._df_all=df_all\n",
        "\n",
        "    assert(df_all is not None)\n",
        "\n",
        "    if (df_train is None):\n",
        "      self._df_train=df_all\n",
        "    else:\n",
        "      self._df_train=df_train\n",
        "    self._df_val=df_val\n",
        "    self._df_df_test=df_test\n",
        "\n",
        "    self.input_features=[]\n",
        "    self.label_features=[]\n",
        "\n",
        "    self.columns_label=[]\n",
        "    self.columns_input=[]\n",
        "\n",
        "    self.global_normalizers={} # Not used so far \n",
        "    self.categorical_columns=[]\n",
        "    self.categories_limit=categories_limit\n",
        "    self.grid=None\n",
        "    self.category_lists= self.__df_desc()\n",
        "      # If a column has less than this number (20 as default) of unique value, I will treate it as a category column.\n",
        "    self.code=\"\"\n",
        "\n",
        "  @classmethod\n",
        "  def get_available_features(cls,col_dtype):\n",
        "    return set(cls.df_dtype_features_cross[[\"Kind\",col_dtype]][cls.df_dtype_features_cross[col_dtype]][\"Kind\"].unique())\n",
        "\n",
        "    # ## To generate normalizer lambda and denormalizer one\n",
        "\n",
        "    # So far, only 2 kinds of normalizer and denormalizer are supported:\n",
        "\n",
        "  # min-max  : (value-min)/(max-min)\n",
        "  # To generate min-max normalizer and denomalizer lambda statements\n",
        "  @staticmethod\n",
        "  def min_max_normalizer(min_v,max_v, v_str=\"by_train\",is_int64=False):\n",
        "    if is_int64:\n",
        "      ext_v_str=\"tf.cast(\"+v_str+\",tf.float32)\"\n",
        "    else:\n",
        "      ext_v_str=v_str\n",
        "    \n",
        "    return \"lambda \"+v_str+\": (\"+ext_v_str+ \" -\"+str(min_v)+\")/(\"+str(max_v)+\"-\"+str(min_v)+\")\",\"lambda \"+v_str+\": \"+ext_v_str+ \" *(\"+str(max_v)+\"-\"+str(min_v)+\")+\"+str(min_v)\n",
        "\n",
        "  # mean-std  : (value-mean)/std\n",
        "  # To generate mean-std normalizer and denomalizer lambda statements\n",
        "  @staticmethod\n",
        "  def std_normalizer(v_mean,v_std, v_str=\"by_train\",is_int64=False):\n",
        "    if is_int64:\n",
        "      ext_v_str=\"tf.cast(\"+v_str+\",tf.float32)\"\n",
        "    else:\n",
        "      ext_v_str=v_str\n",
        "\n",
        "    return \"lambda \"+v_str+\": (\"+ext_v_str+ \" -\"+str(v_mean)+\")/\"+str(v_std),\"lambda \"+v_str+\": \"+ext_v_str+ \" *\"+str(v_std)+\"+\"+str(v_mean)\n",
        "\n",
        "  # To generate min-max/mean-std normalizer and denomalizer lambda statements given an statistics data\n",
        "  @staticmethod\n",
        "  def create_local_normalizers(col_name,df_statistics, v_str=\"by_train\",is_int64=False):\n",
        "    v_min=df_statistics.loc[col_name][\"min\"]\n",
        "    v_max=df_statistics.loc[col_name][\"max\"]\n",
        "    v_mean=df_statistics.loc[col_name][\"mean\"]\n",
        "    v_std=df_statistics.loc[col_name][\"std\"]\n",
        "\n",
        "    n1,d1=TsEstimator.min_max_normalizer(v_min,v_max,v_str,is_int64=is_int64)\n",
        "    n2,d2=TsEstimator.std_normalizer(v_mean,v_std,v_str,is_int64=is_int64)\n",
        "\n",
        "    locals={n1:d1,n2:d2}\n",
        "    return locals\n",
        "\n",
        "  # To generated a suitable string for an integer list\n",
        "  @staticmethod\n",
        "  def int_list_as_string(a):\n",
        "    s = [str(i) for i in a]\n",
        "    return  \"[\"+\",\".join(s)+\"]\"\n",
        "\n",
        "  # To generated a suitable string for a string list\n",
        "  @staticmethod\n",
        "  def string_list_as_string(s):\n",
        "    return  \"['\"+\"','\".join(s)+\"']\"\n",
        "\n",
        "    # ## To generate available feature kinds and suitable normalizer lambda statements for every column.\n",
        "\n",
        "  # Please note the whole dataframe and the train part are both required.\n",
        "\n",
        "  # The whole dataframe is used to decide the vocalbulary list for each column.\n",
        "\n",
        "  # Both the whole dataframe and the train part are used to generate lambda statements for NUMERIC columns. So normalizing can be based on the whole data or only the train part. It's up to the data scientist.\n",
        "\n",
        "  def __df_desc(self):\n",
        "    df_all=self._df_all\n",
        "    df_train=self._df_train\n",
        "\n",
        "    df_statistics_train=df_train.describe().T # I use train part to normalize!\n",
        "    df_statistics_all=df_all.describe().T # I use train part to normalize!\n",
        "    \n",
        "    category_lists={}\n",
        "    \n",
        "    for c in df_train.columns:\n",
        "      dtype_name=df_train[c].dtype.name\n",
        "\n",
        "      availables=self.get_available_features(dtype_name)\n",
        "\n",
        "      if availables is None:\n",
        "        availables={}\n",
        "\n",
        "      feature=\"numeric_column('\"+c+\"')\"\n",
        "\n",
        "      local_normalizers={}\n",
        "\n",
        "      if ((dtype_name==\"int64\") or (dtype_name==\"object\")):\n",
        "        is_int64=(dtype_name==\"int64\")\n",
        "\n",
        "        values_unique=df_all[c].unique()\n",
        "        f=len(values_unique)   # I use all rows to decide the cetegory list   \n",
        "        if f<self.categories_limit: #Category\n",
        "          if is_int64:\n",
        "            feature=categorical_identitys.__name__+\"('\"+c+\"',\"+self.int_list_as_string(values_unique)+\")\"\n",
        "          else:\n",
        "            feature=categorical_strings.__name__+\"('\"+c+\"',\"+self.string_list_as_string(values_unique)+\")\"\n",
        "          self.categorical_columns.append(c)\n",
        "        else:\n",
        "          if is_int64:\n",
        "            feature=\"numeric_column('\"+c+\"')\"\n",
        "            local_normalizers=self.create_local_normalizers(c,df_statistics_train,v_str=\"by_train\", is_int64=True)\n",
        "            self.global_normalizers.update(local_normalizers)\n",
        "            local_normalizers1=self.create_local_normalizers(c,df_statistics_all,v_str=\"by_all\", is_int64=True)\n",
        "            self.global_normalizers.update(local_normalizers1)\n",
        "            local_normalizers.update(local_normalizers1)\n",
        "          else:\n",
        "            feature=\"embedding_column('\"+\"('\"+c+\"')\"\n",
        "      else:\n",
        "        if (dtype_name==\"float64\"):\n",
        "            feature=\"numeric_column('\"+c+\"')\"\n",
        "            local_normalizers=self.create_local_normalizers(c,df_statistics_train,v_str=\"by_train\", is_int64=False)\n",
        "            self.global_normalizers.update(local_normalizers)\n",
        "            local_normalizers1=self.create_local_normalizers(c,df_statistics_all,v_str=\"by_all\", is_int64=False)\n",
        "            self.global_normalizers.update(local_normalizers1)\n",
        "            local_normalizers.update(local_normalizers1)\n",
        "        elif  (dtype_name==\"bool\"):\n",
        "            feature=\"numeric_column('\"+c+\"')\"\n",
        "        elif (dtype_name==\"category\"):\n",
        "          feature=\"categorical_column_with_vocabulary_list('\"+\"('\"+c+\"')\"\n",
        "          self.categorical_columns.append(c)\n",
        "        else:\n",
        "          feature=dtype_defaults[dtype_name] \n",
        "      \n",
        "      availables.add(feature)\n",
        "\n",
        "      availables={s.replace(\"(...)\",\"('\"+c+\"')\") for s in availables}\n",
        "      category_lists[c]={\"default\":feature,\"available\":availables,\"normalizers\": local_normalizers}\n",
        "\n",
        "    return category_lists\n",
        "\n",
        "  def get_feature_grid(self,default_inputs=[], default_labels=[]):\n",
        "    if self.grid is not None:\n",
        "      return self.grid\n",
        "\n",
        "    # category_lists=df_desc(df_all,df_train)\n",
        "    df_all=self._df_all\n",
        "    df_train=self._df_train\n",
        "\n",
        "    cols=len(df_train.columns)\n",
        "    grid = GridspecLayout(cols+1, 12)\n",
        "    # To add a header at row 0\n",
        "    grid[0,0]= widgets.Label(value=\"Column\")\n",
        "    grid[0,1]= widgets.Label(value=\"dtype\")\n",
        "    grid[0,2]= widgets.Label(value=\"Input?\")\n",
        "    grid[0,3]= widgets.Label(value=\"Label?\")\n",
        "    grid[0,4:7]= widgets.Label(value=\"Feature Kind\")\n",
        "    grid[0,8:]= widgets.Label(value=\"Numeric Normalizer\")\n",
        "\n",
        "    for i in range(cols):\n",
        "      feature_option=self.category_lists[df_train.columns[i]]\n",
        "      grid[i+1,0]= widgets.Label(value=df_train.columns[i])\n",
        "      grid[i+1,1]= widgets.Label(value=df_train.dtypes[i].name)\n",
        "      grid[i+1,2]=widgets.Checkbox(value=(df_train.columns[i] in default_inputs),description='',indent=False,layout=Layout(height='auto', width='auto'))\n",
        "      grid[i+1,3]=widgets.Checkbox(value=(df_train.columns[i] in default_labels),indent=False,description='',layout=Layout(height='auto', width='auto'))\n",
        "      \n",
        "      grid[i+1,4:7]= widgets.Dropdown(\n",
        "        options=list(feature_option['available']),\n",
        "        value=feature_option['default'],\n",
        "        description=\"\",\n",
        "        layout=Layout(height='auto', width='auto')\n",
        "        )\n",
        "      \n",
        "      if len(feature_option['normalizers'])>0:\n",
        "        grid[i+1,8:]=widgets.Dropdown(\n",
        "          options=list(feature_option['normalizers'].keys()),\n",
        "          value=list(feature_option['normalizers'].keys())[0],\n",
        "          layout=Layout(height='auto', width='auto'),\n",
        "          description=\"\"\n",
        "          )\n",
        "    \n",
        "    self.grid=grid\n",
        "\n",
        "    return grid\n",
        "\n",
        "    # To generate code based on interactive grid\n",
        "  def __generate_code(self):\n",
        "    code_generator=[]\n",
        "\n",
        "    lambda_1=\"lambda x: x\"\n",
        "\n",
        "    grid=self.grid\n",
        "    for i in range(1,grid.n_rows):\n",
        "      f_col=grid[i,4].value\n",
        "      # print(f_col)\n",
        "      if (grid[i,4].value.startswith(\"numeric_column(\")):\n",
        "        if (grid[i,1].value ==\"bool\"):\n",
        "          lambda_f=lambda_1\n",
        "        else:\n",
        "          lambda_f=grid[i,8].value\n",
        "\n",
        "        # f_col=f_col[:-1]\n",
        "        f_col=grid[i,0].value\n",
        "\n",
        "        if (grid[i,2].value==True):\n",
        "          code_generator.append(\"input_features.append(('\"+f_col+\"',\"+lambda_f+\"))\")\n",
        "          self.columns_input.append(grid[i,0].value)\n",
        "        if (grid[i,3].value==True):\n",
        "          code_generator.append(\"label_features.append(('\"+f_col+\"',\"+lambda_f+\"))\")\n",
        "          self.columns_label.append(grid[i,0].value)\n",
        "      else:\n",
        "        if (grid[i,2].value==True):\n",
        "          code_generator.append(\"input_features.append(\"+f_col+\")\")\n",
        "          self.columns_input.append(grid[i,0].value)\n",
        "        if (grid[i,3].value==True):\n",
        "          code_generator.append(\"label_features.append(\"+f_col+\")\")\n",
        "          self.columns_label.append(grid[i,0].value)\n",
        "    return code_generator, self.columns_label    \n",
        "\n",
        "  def __run_generated_code(self, code_generator):\n",
        "    code=';'.join(code_generator)\n",
        "    # print(code)\n",
        "\n",
        "    try:\n",
        "      self.input_features.clear()\n",
        "      self.label_features.clear()\n",
        "      exec(code,None, {'input_features':self.input_features,'label_features':self.label_features})\n",
        "      print(\"The feature_columns have been generated!\")\n",
        "    except:\n",
        "      print(\"Please check the generated code\", sys.exc_info()[0])\n",
        "    # print(code_generator)\n",
        "\n",
        "  def update_by_grid(self):\n",
        "    self.code,_ =self.__generate_code()\n",
        "    # print(\"code:\",code)\n",
        "    self.__run_generated_code(self.code)\n",
        "\n",
        "  def input_label_1d(self):\n",
        "    def transform(row):\n",
        "      i_result1=[tf.reshape(tf.cast(y(row[x]),tf.float64),[-1]) for (x, y) in (self.input_features) ]\n",
        "      i_result2=[tf.reshape(tf.cast(y(row[x]),tf.float64),[-1]) for (x, y) in (self.label_features) ]\n",
        "\n",
        "      return tf.concat(i_result1+i_result2,0)\n",
        "    return transform\n",
        "\n",
        "  def label_1d(self):\n",
        "    def transform(row):\n",
        "      i_result2=[tf.reshape(tf.cast(y(row[x]),tf.float64),[-1]) for (x, y) in (self.label_features) ]\n",
        "\n",
        "      return tf.concat(i_result2,0)\n",
        "    return transform\n",
        "\n",
        "  @staticmethod\n",
        "  def split_input_label(forecast_size,v_labels, single_step=False):\n",
        "    def input_label(row):\n",
        "      i_input=row[:-forecast_size,:]\n",
        "\n",
        "      if single_step:\n",
        "        l_label=row[-1,-v_labels:]\n",
        "      else:\n",
        "        i_label=tf.reshape(row[-forecast_size:,-v_labels:],[-1]) # To reshape to a 1-d tensor\n",
        "\n",
        "      return i_input, i_label\n",
        "\n",
        "    return input_label\n",
        "\n",
        "    # A utility method to create a tf.data dataset from a Pandas Dataframe\n",
        "  def df_to_dataset(self, dataframe,past_history=5, future_target=2, shuffle=False, batch_size=32):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(dict(dataframe)) \n",
        "\n",
        "    ds_map=ds.map(self.input_label_1d())\n",
        "\n",
        "    v_labels=ds.map(self.label_1d()).element_spec.shape[-1]\n",
        "\n",
        "    # Feel free to play with shuffle buffer size\n",
        "    shuffle_buffer_size = len(dataframe)\n",
        "    # Total size of window is given by the number of steps to be considered\n",
        "    # before prediction time + steps that we want to forecast\n",
        "\n",
        "    total_size = past_history + future_target\n",
        "\n",
        "    # Selecting windows\n",
        "    data = ds_map.window(total_size, shift=1, drop_remainder=True)\n",
        "    data = data.flat_map(lambda k: k.batch(total_size))\n",
        "\n",
        "    # Shuffling data (seed=Answer to the Ultimate Question of Life, the Universe, and Everything)\n",
        "    if shuffle:\n",
        "      data = data.shuffle(shuffle_buffer_size, seed=42)\n",
        "\n",
        "    # Extracting past features  + labels\n",
        "    data = data.map(TsEstimator.split_input_label(future_target,v_labels))\n",
        "\n",
        "    ds_4_train= data.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return ds_4_train\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zVh-ESps9u1",
        "colab_type": "text"
      },
      "source": [
        "## <font color=red> Your Dataframe here</font>\n",
        "Typically, this is the <font color=red>ONLY</font> place for you to type.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYTERy0Js85O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csvURL = '' # the csv data file or web path\n",
        "\n",
        "default_inputs=[]  # The default features list for inputs\n",
        "default_labels=[] # The default features list for labels\n",
        "\n",
        "dataframe=None\n",
        "if (csvURL!=''):\n",
        "  dataframe = pd.read_csv(csvURL)\n",
        "  dataframe.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG_6nILluUe_",
        "colab_type": "text"
      },
      "source": [
        "## A demo dataframe if you don't create one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TokBlnUhWFw9"
      },
      "source": [
        "This tutorial uses a <a href=\"https://www.bgc-jena.mpg.de/wetter/\" class=\"external\">[weather time series dataset</a> recorded by the <a href=\"https://www.bgc-jena.mpg.de\" class=\"external\">Max Planck Institute for Biogeochemistry</a>.\n",
        "\n",
        "This dataset contains 14 different features such as air temperature, atmospheric pressure, and humidity. These were collected every 10 minutes, beginning in 2003. For efficiency, you will use only the data collected between 2009 and 2016. This section of the dataset was prepared by François Chollet for his book [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qoFJZmXBaxCc"
      },
      "source": [
        "In both the following tutorials, the first 300,000 rows of the data will be the training dataset, and there remaining will be the validation dataset. This amounts to ~2100 days worth of training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "REZ57BXCLdfG",
        "colab": {}
      },
      "source": [
        "if (dataframe is None):\n",
        "  zip_path = tf.keras.utils.get_file(\n",
        "      origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "      fname='jena_climate_2009_2016.csv.zip',\n",
        "      extract=True)\n",
        "  csv_path, _ = os.path.splitext(zip_path)\n",
        "  dataframe = pd.read_csv(csv_path,index_col='Date Time')\n",
        "  default_inputs=['p (mbar)', 'T (degC)', 'rho (g/m**3)']  # The default features list for inputs\n",
        "  default_labels=['T (degC)'] # The default features list for labels\n",
        "\n",
        "dataframe.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDQ9-v7WxJp7",
        "colab_type": "text"
      },
      "source": [
        "## Split data into Train and Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YEOpw7LhMYsI",
        "colab": {}
      },
      "source": [
        "dataframe_train, dataframe_test = train_test_split(dataframe, test_size=0.2)\n",
        "dataframe_train, dataframe_val = train_test_split(dataframe_train, test_size=0.2)\n",
        "print(len(dataframe_train), 'train examples')\n",
        "print(len(dataframe_val), 'validation examples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqPxNqGNQpLN",
        "colab_type": "text"
      },
      "source": [
        "## Create an estimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "580l-ZQhPP5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator=TsEstimator(dataframe, dataframe_train, dataframe_val, dataframe_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IWdQBbSMXUL",
        "colab_type": "text"
      },
      "source": [
        "## Inspect the data by categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73VFzUHpMYph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if len(estimator.categorical_columns)>0:\n",
        "  # Use seaborn for pairplot\n",
        "  !pip install -q seaborn\n",
        "  # import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "  # plt.figure(figsize=(20,5))\n",
        "  sns.pairplot(dataframe[estimator.categorical_columns], diag_kind=\"kde\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7BshfvV0X1g",
        "colab_type": "text"
      },
      "source": [
        "## To create an interactive grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0grSPtD7QBv",
        "colab_type": "text"
      },
      "source": [
        "You may try the builder INTERACTIVELY."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_SWf5pRPYtn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid=estimator.get_feature_grid(default_inputs, default_labels)\n",
        "grid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj0RBVpb7b6N",
        "colab_type": "text"
      },
      "source": [
        "<font color=red>**RERUN** the following cells once you change the above settings.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PIYVldRC6sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator.update_by_grid()\n",
        "\n",
        "assert(len(estimator.input_features)>0)\n",
        "assert(len(estimator.label_features)>0)\n",
        "# code_generator\n",
        "estimator.input_features,estimator.label_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MklfupgPOjum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x,y in estimator.input_features:\n",
        "  print(x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79J6snPuPZ1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator.code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "84ef46LXMfvu"
      },
      "source": [
        "## Create an input pipeline using tf.data\n",
        "\n",
        "Next, I will wrap the dataframes with [tf.data](https://www.tensorflow.org/guide/datasets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CXbbXkJvMy34",
        "colab": {}
      },
      "source": [
        "past_history = 720\n",
        "future_target = 72\n",
        "batch_size = 5 # A small batch sized is used for demonstration purposes\n",
        "single_step=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZVypiSMfSAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a_past_history=widgets.Label(\"Past History Periods:\")\n",
        "v_past_history = widgets.IntText(value=past_history)\n",
        "box_past_history = widgets.HBox([a_past_history, v_past_history])\n",
        "\n",
        "a_future_target=widgets.Label(\"Future_target Periods:\")\n",
        "v_future_target = widgets.IntText(value=future_target)\n",
        "box_future_target = widgets.HBox([a_future_target, v_future_target])\n",
        "\n",
        "a_batch_size=widgets.Label(\"Batch Size:\")\n",
        "v_batch_size = widgets.IntText(value=batch_size)\n",
        "box_batch_size = widgets.HBox([a_batch_size, v_batch_size])\n",
        "\n",
        "a_single_step=widgets.Label(\"Single Step?\")\n",
        "v_single_step= widgets.Checkbox(value=single_step)\n",
        "\n",
        "grid = widgets.GridspecLayout(4, 5)\n",
        "\n",
        "grid[0,0]=a_past_history; grid[0,1:]=v_past_history\n",
        "grid[1,0]=a_future_target; grid[1,1:]=v_future_target\n",
        "grid[2,0]=a_batch_size; grid[2,1:]=v_batch_size\n",
        "grid[3,0]=a_single_step; grid[3,1:]=v_single_step\n",
        "\n",
        "grid\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlAf4PI4lp-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "past_history = v_past_history.value\n",
        "future_target = v_future_target.value\n",
        "batch_size = v_batch_size.value # A small batch sized is used for demonstration purposes\n",
        "single_step=v_single_step.value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00Y1gvFXfwqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = estimator.df_to_dataset(dataframe_train,past_history=past_history, future_target=future_target,  shuffle=False, batch_size=32)\n",
        "val_ds = estimator.df_to_dataset(dataframe_val,past_history=past_history, future_target=future_target, shuffle=False, batch_size=32)\n",
        "test_ds = estimator.df_to_dataset(dataframe_test,past_history=past_history, future_target=future_target, shuffle=False, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mxwiHFHuNhmf",
        "colab": {}
      },
      "source": [
        "next(iter(train_ds))[0].shape,next(iter(train_ds))[1].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bBx4Xu0eTXWq"
      },
      "source": [
        "## Create, compile, and train the model\n",
        "\n",
        "Slightly modified from Multi-Step model for a multivariate time series in https://www.tensorflow.org/tutorials/structured_data/time_series\n",
        "\n",
        "Depends on your target, you may need to change the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE440FOUFhdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape=next(iter(train_ds))[0].shape[-2:]\n",
        "label_shape=next(iter(train_ds))[1].shape[-1]\n",
        "input_shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftri-0Rsyi2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EVALUATION_INTERVAL = 200\n",
        "multi_step_model = tf.keras.models.Sequential()\n",
        "multi_step_model.add(tf.keras.layers.LSTM(32,\n",
        "                                          return_sequences=True,\n",
        "                                          input_shape=input_shape))\n",
        "multi_step_model.add(tf.keras.layers.LSTM(16, activation='relu'))\n",
        "multi_step_model.add(tf.keras.layers.Dense(label_shape))\n",
        "\n",
        "multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')\n",
        "\n",
        "multi_step_history = multi_step_model.fit(train_ds, epochs=5,\n",
        "                                          steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                                          validation_data=val_ds,\n",
        "                                          validation_steps=50)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}